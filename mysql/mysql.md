# MySql架构

## 连接MySql大概过程

![image-20210413151039244](img/image-20210413151039244.png)

> `JDBC`流程如下
>
> ~~~java
>  public static void main(String[] args) throws SQLException, ClassNotFoundException {
>         Class.forName("com.mysql.cj.jdbc.Driver");
>         Connection conn = DriverManager.getConnection("jdbc:mysql://127.0.0.1:3306/imooc", "root", "root");
>         Statement statement = conn.createStatement();
>         statement = statement;
>         String sql = "insert into user(loginName,userName,password,sex)values('tom123','tom','123456',1)";
>         int result = statement.executeUpdate(sql);
>   }
> ~~~



网络连接由线程处理：当数据库服务器连接池接受到网络请求时，会分配一个工作线程去进行处理

## MySql处理流程

![image-20210414111703139](img/image-20210414111703139.png)





### SQL接口：负责处理接收到的SQL语句

​	MySql工作线程从网络连接中读取出SQL语句后，会将其交给SQL接口去执行。

​	**SQL接口**：`是一套执行SQL语句的接口，专门用于执行我们发送给MySql的增删改查语句`

### 查询解析器：让MySql看懂sql语句

​	**Parser(查询解析器)**：`按照既定的sql语法规则，对sql语句进行解析，理解sql语句要做什么事情`

### 查询优化器：选择最优查询路径

​	**Optimizer(查询优化器)**：`针对编写的sql，生成所谓查询路径树，从中选择一条最优路径出来`

​	相当于告诉你，你应该按照什么样的步骤和顺序，去执行那些操作，然后一步一步的把sql语句给完成

### 执行器：根据执行计划调用存储引擎接口

​	`执行器会根据优化器生成的一套执行计划，然后不停的调用存储引擎的各种接口去完成sql语句的执行计划`

### 存储引擎接口：真正执行sql语句

​	MySql的架构设计中，SQL接口，解析器，优化器都是通用的，仅是一套组件而已。但存储引擎是由各种各样的，如InnoDB，MyISAM等，我们是可以选择使用那种存储引擎来负责具体sql执行的。

​	`存储引擎就是执行sql语句的，他会按照一定步骤去查询内存缓存数据，更新磁盘数据，查询磁盘数据等等`





# InnoDB

​	以一条update语句为例，初步了解InnoDB存储引擎的架构设计

​	update user set name = 'xxx' where id = 10	这条sql首先会通过数据库连接发送到MySql上，然后经过SQL接口，解析器，优化器，执行器几个环节，解析SQL语句，生成执行计划，接着由执行器负责计划的执行，调用InnoDB存储引擎的接口去执行

## 更新语句大致流程

---



### 	缓存池（Buffer Pool）

​		引擎执行更新语句时，会先判断id=10的数据是否存在于缓冲池中。如果不存在，则会从磁盘中加载数据，并且会对这行记录加独占锁

> Buffer Pool是一个放在内存中的组件，非常重要。其中放有缓存的数据，便于以后查询时，可以考虑直接从缓冲中获取。

### 	undo日志

​		如果id为10的数据name为zhangsan，先将其更新为xxx。那需要把这条记录原来的值（zhangsan，id为10）这些信息，写入到undo日志文件中去

> 考虑到未来可能要回滚数据，需要将更新前的值写入到undo日志文件中

### 更新buffer pool中数据

​		在将磁盘文件中加载到缓冲池后，同时加锁之后，并将修改前的数据写入到undo日志文件中之后。就可以正式更新这行记录了。更新时，先更新buffer pool中的数据。此时缓存数据和磁盘数据不一致

### Redo Log Buffer

​		为了避免MySql宕机，导致内存中数据丢失，需要把对内存所作修改写入到Redo Log Buffer中去，这也是内存中的一个缓冲区，使用了存放redo日志的。``redo日志就是记录对数据做了什么修改``

​		如果此时MySql宕机，此时内存中的数据全部丢失，但是由于事务并没有更新，所以磁盘上的数据依旧是旧数据

### Redo落盘

​		提交事务时，需要根据一定策略将redo日志从redo log buffer中刷入磁盘。这个策略是通过 **innodb_flush_log_at_trx_commit** 来进行配置的。其刷盘策略有三种

> 参数为0：提交事务时，不会将redo log buffer中数据刷入磁盘。如果此时MySql宕机了，那么内存中数据会丢失
>
> 参数为1：提交事务时，必须把redo log从内存中刷到磁盘文件中去，只要提交事务成功，那么redo log就必然在磁盘里了。此时MySql若宕机，那么内存中的数据虽然丢失，但是redo log 文件得到保存，MySql可以根据redo日志去进行恢复
>
> 参数为2：提交事务时，把redo日志写入到磁盘os cache缓存中，等待刷入到磁盘中去。如此时MySql宕机，并且redo文件还停留带os cacha中，那么数据会丢失

​		因此，redo日志刷盘策略一般都选为1，保证事务提交之后，数据不会丢失

### Binlog日志

> ​	redo log是一种**偏向物理性质的重做日志**，他里面记录的是类似这样的东西：对那个数据页中的什么记录，进行了什么修改。并且redo log是InnoDB存储引擎特有的
>
> ​	而binlog叫归档日志，**记录的是偏逻辑性的日志**，类似于：对user表中的id=10的行数据进行了更新，更新后的值是什么，binlog是mysql server自己的日志文件，不是InnoDB所特有的

​	在提交事务的同时，也会把这次更新对应的binlog文件写入到磁盘中去。对于binlog也有不同的刷盘策略，由参数 **sync_binlog** 控制。

> 参数为0：提交事务时，将binlog日志写入磁盘os cache中，等待刷入磁盘。此时若宕机，数据会丢失。0为默认值
>
> 参数为1：提交事务时，强制将binlog日志写入到磁盘中去。

### 基于binlog和relo log完成事务提交

​		当binlog写入到磁盘文件中后，接着就完成最终的事务提交，此时会将更新对应的binlog文件名称和此次更新的binlog日志在文件中的位置都写入到redo log日志文件中去，同时在redo log日志文件里写入一个commit标记。此时，才算完成了事务的提交

​		在redo log中写入commit标记的意义在于：`保持redo log和binlog的一致。`因为只有redo log和binlog一致，才能确认本次事务提交成功。

### 后台IO线程将内存中数据刷入磁盘

​		MySql后台有一个IO线程，会在之后某时间，随机的将内存buffer pool中的修改后的脏数据给刷回到磁盘中的数据文件中去。哪怕宕机也不影响，因为redo log和binlog中留有记录，可自行恢复





![image-20210415151815139](img/image-20210415151815139.png)



**执行器是非常核心的一个组件，负责跟存储引擎配合完成一个SQL语句在磁盘和内存层面的全部数据更新操作**

在执行更新时，每条SQL语句，都会对应修改buffer pool中的缓存数据，写undo日志，写redo log buffer 等步骤；在提交事务时，一定会将redo log刷入磁盘，binlog刷入磁盘，完成redo log中的commit标记；最后，再由后台IO线程随机的将buffer pool中的脏数据刷入到磁盘中





# 生产层面

8核16G，一般每秒扛一两千请求是没有问题的

16核32G，每秒三四千一般也没问题

数据库的磁盘最后使用SSD固态硬盘（数据库需要执行IO操作）



QPS(QUERY PER SECOND)：每秒处理多少请求。对数据库而言，可理解为每秒处理多少SQL语句

TPS(TRANSCATION PER SECOND)：每秒可处理的事务量。可理解为每秒处理多少事务提交或回滚

## 压测指标

### IO相关

- **IOPS**：这个指的是机器的随机IO并发处理的能力，比如机器可以达到200 IOPS，意思就是说每秒可以执行200个随机IO读写请求。

	这个指标是很关键的，因为之前我们在数据库架构原理中讲解过，你在内存中更新的脏数据库，最后都会由后台IO线程在不确定的时间，刷回到磁盘里去，这就是随机IO的过程。如果说IOPS指标太低了，那么会导致你内存里的脏数据刷回磁盘的效率就会不高。

- **吞吐量**：指机器的磁盘存储每秒可以读写多少字节的数据量

	这个指标也是很关键的，因为大家通过之前的学习都知道，我们平时在执行各种SQL语句的时候，提交事务的时候，其实都是大量的会写redo log之类的日志的，这些日志都会直接写磁盘文件。所以一台机器他的存储每秒可以读写多少字节的数据量，就决定了他每秒可以把多少redo log之类的日志写入到磁盘里去。一般来说我们写redo log之类的日志，都是对磁盘文件进行顺序写入的，也就是一行接着一行的写，不会说进行随机的读写，那么一般普通磁盘的顺序写入的吞吐量每秒都可以达到200MB左右。

- **latency**：指向磁盘中写入一条数据的延迟

	这个指标同样很重要，因为我们执行SQL语句和提交事务的时候，都需要顺序写redo log磁盘文件，所以此时你写一条日志到磁盘文件里去，到底是延迟1ms，还是延迟100us，这就对你的数据库的SQL语句执行性能是有影响的。一般来说，当然是你的磁盘读写延迟越低，那么你的数据库性能就越高，你执行每个SQL语句和事务的时候速度就会越快。

### 其他指标

- **CPU负载**：CPU负载是一个很重要的性能指标，因为假设你数据库压测到了每秒处理3000请求了，可能其他的性能指标都还正常，但是此时CPU负载特别高，那么也说明你的数据库不能继续往下压测更高的QPS了，否则CPU是吃不消的。
- **网络负载**：这个主要是要看看你的机器带宽情况下，在压测到一定的QPS和TPS的时候，每秒钟机器的网卡会输入多少MB数据，会输出多少MB数据，因为有可能你的网络带宽最多每秒传输100MB的数据，那么可能你的QPS到1000的时候，网卡就打满了，已经每秒传输100MB的数据了，此时即使其他指标都还算正常，但是你也不能继续压测下去了。
- **内存负载**：这个就是看看在压测到一定情况下的时候，你的机器内存耗费了多少，如果说机器内存耗费过高了，说明也不能继续压测下去了。

### 压测工具和监控

​	[压测工具sysbench](https://apppukyptrl1086.pc.xiaoe-tech.com/detail/i_5e383c5357307_MjhluwMb/1?from=p_5e0c2a35dbbc9_MNDGDYba&type=6)

​	[监控工具Prometheus和Grafana](https://apppukyptrl1086.pc.xiaoe-tech.com/detail/i_5e398efe3b8f9_XZOAxAmQ/1?from=p_5e0c2a35dbbc9_MNDGDYba&type=6)

​	

# Buffer Pool

buffer pool是数据库中必须要搞懂的一个核心组件，因为增删改查操作首先就是针对这个内存中的Buffer Pool里的数据执行的，同时配合了后续的redo log，刷磁盘等机制和操作

## Buffer Pool的数据结构

Buffer Pool默认大小为128MB，可以通过配置修改。本质就是数据库中的一个内存组件，由一大堆缓存页和描述数据块组成，然后加上各种链表(free,flush,lru)辅助其运行

~~~shell
[server]
innodb_buffer_pool_size=2147483648
~~~



### 数据页和缓存页

MySql对数据抽象出了一个**数据页**的概念，他将很多**行数据**放在了一个数据页中，每一页数据中放了很多行数据。所以buffer pool中的存放的是一个个的数据页

默认情况下，磁盘中存放的数据页的大小是16KB，也就是说一页数据包含了16KB的内容。Buffer Pool中存放的页数据又称之为缓存页，一个缓存页的大小和一个数据页的大小是一一对应的。

### 缓存页中的描述信息

对于每个**缓存页**，都会有一个**描述信息**，这个描述信息可以大体确认是用来描述这个缓存页的，一般包含：这个数据页的表空间，数据页的编号，这个缓存页在Buffer Pool中的内存地址以及一些别的信息。这些描述信息本身也是一块数据，在Buffer Pool中，每个缓存页的描述数据放在最前面，然后各个缓存页放在后面。如下图

![image-20210415165620384](img/image-20210415165620384.png)

Buffer Pool中的描述数据大概相当于缓存页的大小的5%左右。假设设置的buffer pool大小是128MB，那么实际上真正的大小可能有130MB，因为他里面还含有每个缓存页中的数据

### Buffer Pool的初始化

数据库启动时，便会按照配置的buffer pool大小，再稍微加大一点，去向操作系统申请一块内存空间，作为buffer pool的内存区域。内存区域申请完毕后，数据库就会按照默认的缓存页的16KB大小和对应的800字节左右的描述数据的大小，在Buffer Pool中划分出一个一个的缓存页和对于的描述数据。但此时缓存页都是空的，什么都没有

## Free链表

### 链表结构

Buffer Pool中有一个**Free链表**设计，他是一个**双向链表结构**，每个节点就是一个**空闲缓存页的描述数据块的地址**，即只要有一个缓存页是空闲的，那么他的描述数据块就会被放入到这个free链表中

![image-20210415172352892](img/image-20210415172352892.png)

free链表中还有一个基础节点，指向链表的头尾节点，里面还存储了链表中有多少个描述数据块的节点，即空闲缓存页

### 占用空间

free链表本身，就是由Buffer Pool中的描述数据块组成，每个描述数据块中都有两块指针，free_pre和free_netx。分别指向自己的上一个free链表的节点，以及下一个free链表的节点。

对于free链表而言，只有一个基础节点是不属于Buffer Pool的，他是40字节大小的一个节点，里面存放了free链表的头节点的地址，尾节点的地址，还有free链表中当前有多少节点

### 将数据读入到Buffer Pool

1. 首先从free链表中获取一个描述数据块，并根据数据库找到缓存页
2. 将磁盘上的数据读取到对应的缓存页中去，同时将相关描述信息写入到缓存页的描述数据块中。比如数据页所属表空间等信息。
3. 最后把描述数据库从free链表中去除

### 判断数据页是否缓存

`数据库中还会有一个哈希表数据结构，他会用表空间+数据页号，作为一个key，然后缓存页地址作为value`

当要使用数据页的时候，通过“表空间+数据页号”作为key去哈希表中查询一下，如果没有就读取数据页，如果有了，就说明已经缓存了

![image-20210416150325751](img/image-20210416150325751.png)

## Flush链表

结构和free链表类似，也是通过缓存页的描述数据块中的两块指针，让被修改过的缓存页的描述数据块，组成一个双向链表。凡是修改过的缓存页，都会将其数据块加入到flush链表中。flush的意思就是脏页，后续都是要将其flush刷新到磁盘上去的。

设计flush链表就是为了能够快速的将脏数据页中的数据刷回到磁盘中去。在更新缓存页的时候，通过变换缓存页中的描述数据块flush链表的尾指针，就可以把脏页的描述数据块组成一个双向链表，即flush链表，而且flush链表的基础节点会指向起始节点和尾巴节点。



## 缓存页的淘汰

如果所有的缓存页都被塞了数据，此时无法从磁盘上加载新的数据页到缓存页中去。那么就要淘汰一些缓存页——`即把一个缓存中修改过的数据，刷入到磁盘中去，然后将缓存页清空，再把新的数据页加载到空闲的缓存页中去`

### 缓存命中率以及LRU链表

淘汰缓存页时应尽可能选择使用率较少的缓存页，因此需要知道那些缓存页经常被访问，那些缓存页很少被访问。此时需要引入**LRU链表(Least Recently Used，最少使用)。**其数据结构和之前的Free链表，Flush链表类似，都是一个双向链表，节点均指向缓存页的描述数据块的地址



### 原始的LRU淘汰机制

> 大致工作原理：当我们从磁盘中**加载一个数据页到缓存页**时，就将这个缓存页的描述数据放入到LRU链表头部去，那么只有有数据的缓存页，都会在LRU链表中，而且最近被加载数据的缓存页，都会放到LRU链表的头部去。假设某个缓存页的描述数据在LRU链表的尾部，后续只要查询了或修改了这个缓存页，也会把这个缓存页挪动到LRU链表的头部，也就是**最近被访问过的缓存页，一定在LRU链表头部**
>
> 因此需要淘汰缓存页时，可以优先从LRU链表尾部进行淘汰

但这样的方式因为MySql的预读机制存在巨大隐患。

预读机制：当从磁盘上加载数据页的时候，MySql会连带着把这个数据页的相邻的其他数据页，都加载到缓存中去。

![image-20210416154211877](img/image-20210416154211877.png)

如果没有空闲缓存页了，此时加载新的数据页，就会将LRU链表尾部的数据淘汰。但这样是不合理的，毕竟其数据是有人访问的，而预读机制带入的数据页可能无人访问

> 一般而言，有两种情况可能触发MySql的预读机制
>
> - 有一个参数是innodb_read_ahead_threshold，他的默认值是56，意思就是如果顺序的访问了一个区里的多个数据页，访问的数据页的数量超过了这个阈值，此时就会触发预读机制，把下一个相邻区中的所有数据页都加载到缓存里去
> - 如果Buffer Pool里缓存了一个区里的13个连续的数据页，而且这些数据页都是比较频繁会被访问的，此时就会直接触发预读机制，把这个区里的其他的数据页都加载到缓存里去。这个机制是通过参数innodb_random_read_ahead来控制的，他默认是OFF，也就是这个规则是关闭的
>
> 预读机制可以一定程度上优化性能。如果你顺序读取很多数据页，MySql会判断可能会接着顺序读取后面的数据页。那么其提前读取到Buffer Pool中，后续读取就会方便很多

还有一种可能导致频繁被访问的缓存页被淘汰的情景：**全表扫描**（select *）

此时如果没where条件，那么会将表里全部数据页都从磁盘加载到Buffer Pool中去。此时LRU链表尾部，可能全部都是之前经常被访问的缓存页。此时进行淘汰时，就会将频繁访问的数据页给淘汰掉。

### 基于冷热数据分离思想设计LRU

真正的MySql在设计LRU链表时，采取的实际是**冷热数据分离**的思想。之前的问题，都是由于所有缓存页全部混在一个LRU链表中导致的

真正的LRU链表，会被拆分为两个部分，一部分是热数据，一部分是冷数据。冷热数据比例由**innodb_old_blocks_pct**参数控制的，默认是37，即冷数据占比37%

**当数据页第一次加载到缓存时**：数据页第一次被加载到缓存时，缓存页会被放在**冷数据区域的链表头部**

**冷数据区域缓存何时被放入热数据区域**：**innodb_old_blocks_time 参数**，默认值是1000，即1000ms。也就是说必须是一个数据页被加载到缓存页之后，在1s之后，访问这个缓存页，才会被挪动到热数据区域的链表头部去

![image-20210416161307127](img/image-20210416161307127.png)

这样设计的话，预读机制以及全表扫描加载进来的一大批缓存页，都会放在LRU链表的冷数据区域的前面位置。而那些频繁访问的缓存页都被放在热数据区域。淘汰缓存时，直接淘汰冷数据区域尾部的缓存页，刷入磁盘即可。

在这样的设计机制下，原始的LRU淘汰机制的问题基本都被解决掉了。

`因为那种预读机制以及全表扫描机制加载进来的数据页，大部分都会在1s之内访问一下，之后可能就再也不访问了，所以这种缓存页基本上都会留在冷数据区域里。然后频繁访问的缓存页还是会留在热数据区域里。当你要淘汰缓存的时候，优先就是会选择冷数据区域的尾部的缓存页，这就是非常合理的了！他不会让刚加载进来的缓存页占据LRU链表的头部，频繁访问的缓存页在LRU链表的尾部，淘汰的时候淘汰尾部的频繁访问的缓存页了！`

**热数据区域的优化**：LRU链表中的热数据区域的访问规则进行了优化，即只有在热数据区域的后3/4区域的缓存页被访问了，才会移动到链表头部去，如果是前面1/4的区域被访问，是不会移动到链表头部的。这样减少了链表的移动。

### 定时将LRU尾部缓存页刷入磁盘

并不是在缓存页满时，才会将LRU冷数据区域尾部的缓存页刷入磁盘。而是有一个后台线程，会运行一个定时任务，每隔一段时间就会把LRU链表的冷数据尾部的缓存页刷入到磁盘中，清空缓存页，并将其加入Free链表中

### 把Flush链表的缓存页刷入磁盘

仅将LRU冷数据区域缓存页刷入磁盘是不够的，后台线程也会在**MySql不繁忙的时候，将flush链表中的缓存页刷入磁盘中**，这样修改过的数据，迟早也会刷入磁盘。只要flush链表的缓存页被刷入磁盘，那么缓存页也会从flush链表和lru链表中移除，然后加入到free链表中



### 总结

总之，MySQL在执行CRUD的时候，首先就是大量的操作缓存页以及对应的几个链表。然后在缓存页都满的时候，必然要想办法把一些缓存页给刷入磁盘，然后清空这几个缓存页，接着把需要的数据页加载到缓存页里去！

你的MySQL的内核参数，应该如何优化，优化哪些地方的行为，才能够尽可能的避免在执行CRUD的时候，经常要先刷一个缓存页到磁盘上去，才能读取一个磁盘上的数据页到空闲缓存页里来？

> 如果要避免上述问题，就要避免缓存页频繁的使用完毕。这里的关键点就在于buffer pool的大小（程序对于缓存页是使用，以及定时线程释放缓存页的过程很难去控制）
>
> buffer pool的大小以及buffer pool的数量，这些参数需要用心的设置和优化，因为他对MySql的性能和并发能力，由较大的影响。



## Buffer Pool的优化

### 多个Buffer Pool

多线程并发访问Buffer Pool时，如果访问的是一些共享数据，那么必然会进行锁操作。因此如果并发请求高的话，性能会受到影响

因此可以设置多个Buffer Pool来优化并发能力。MySql默认规则是，如果给Buffer Pool分配的内存小于1GB，那么就会分配一个Buffer Pool。如果分配的内存大，那么就可以设置多个Buffer Pool

~~~shell
# buffer pool总大小为8GB，4个buffer pool实例
[server]
innodb_buffer_pool_size = 8589934592
innodb_buffer_pool_instances = 4
~~~

这样多线程并发性能就会得到很大的提高

![image-20210416173751720](img/image-20210416173751720.png)

### 基于chunk机制动态调整Buffer Pool大小（不常用）

Buffer Pool是由很多**chunk**组成的，每个大小是由 **innodb_buffer_pool_chunk_size** 参数控制的，默认值是128MB。每个Buffer Pool里的每个chunk里就是一系列的描述数据块和缓存页，多个chunk共享一套free,flush,lru链表

![image-20210416174234597](img/image-20210416174234597.png)

基于chunk机制，就可以动态的调整buffer pool的大小了。如果需要增加Buffer Pool的大小，那么可以申请一系列的128MB大小的chunk就可以了，只要每个chunk是连续到128MB内存就可以，然后将申请到的chunk内存分配给Buffer Pool即可



## Buffer Pool的参数设置